{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Viola-Jones Algorithm for Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viola - Jones Algorithm \n",
    "# 1. Training and Detection\n",
    "# designed to  detect frontal faces - not sides or up \n",
    "# we use gray scale images - easier to use with gray scale images\n",
    "\n",
    "# boxes of different sizes are tried over different patches of the image. Patch or area of image where each feature required \n",
    "# for a face is there will be a high potential candidate for a face.\n",
    "# Paul Viola and Michael Jones (2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haar-like Features\n",
    "# edge feaures (eyebrows, can be nose line ), line features (mouth, nose line, eyes) , four-rectangle features\n",
    "# Above features should be typically present in a face \n",
    "\n",
    "# we might get places where we will get haar like features which is similar to of lets say nose but with the help of other \n",
    "# features it will decide whether that patch is a face or not \n",
    "\n",
    "# to calculate haar liek features - we get the pixel intensities and then average it for both the types and get a difference of\n",
    "# it, then we have some thershold kto lassify that as haar like features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integral Image - of same size of original image - USED TO CALCULATE HAAR LIKE FEATURES EFFICIENTLY\n",
    "# each cell A(i, j) will sum of all the pixel intensities of matrix of dimension (i, j) and A(i, j) will be at bottom corner \n",
    "# of that matrix\n",
    "\n",
    "# Once integral image is calculated, we just have to do four operations to find each haar like features\n",
    "# this speeds us viola - jones algo a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING IN VIOLA - JONES IMAGE detection - \n",
    "# image is shrinked to 24x24 image, then the features are looked for \n",
    "# in actual paper - 4916 images were used to train the algo. - face imgages \n",
    "# 9544 non-face images - they were not 24x24 images and bigger - 350 million subwiindows we had in VJ algo\n",
    "# constantine p. papageorgiou (1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE BOOSTING (ADABOOST)\n",
    "# even a small image of 24x24 will have almost 180k different features, which is very huge\n",
    "# this is a very big problem while training and also while detecting ace real time \n",
    "# ENSEMBLE - POWER OF THE CROWD \n",
    "# Boosting Image Retrieval - Kinh Tieu and Paul Viola (2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASCADING \n",
    "# take a subwindow, take top 5 features and see if those features are there in that subwindow, if not present reject \n",
    "# the subwindow\n",
    "# if all of them are present then check for second set of features. if they are not present, then reject \n",
    "# and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the cascades\n",
    "\n",
    "# How do we create a cascades ?\n",
    "face_cascade = cv2.CascadeClassifier(\"./data/haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(\"./data/haarcascade_eye.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cascade works for black and white image\n",
    "# function which is gonna do the detections\n",
    "def detect(gray, frame):\n",
    "    # Detects objects of different sizes in the input image. The detected objects are returned as a list of rectangles.\n",
    "    # faces are tuples - x and y which is top left point and width and height\n",
    "    faces = face_cascade.detectMultiScale(image=gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    # iterate thorugh the faces \n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw a recatangle on the face \n",
    "        cv2.rectangle(img=frame, pt1=(x, y), pt2=(x+w, y+h), color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        # getting the region of interest for eyes\n",
    "        roi_gray = gray[y:y+h, x:x+h]\n",
    "        roi_frame = frame[y:y+h, x:x+h]\n",
    "        \n",
    "        # detecting eyes\n",
    "        eyes = eye_cascade.detectMultiScale(image=roi_gray, scaleFactor=1.1, minNeighbors=3)\n",
    "        \n",
    "        \n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "             # draw the rectangles for eyes\n",
    "            cv2.rectangle(img=roi_frame, pt1=(ex, ey), pt2=(ex+ew, ey+eh), color=(0, 255, 0), thickness=2)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # get the last frame from the webcam\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    # We do some colour transformation\n",
    "    # black and white version of the image \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # we get the output of our detect function\n",
    "    canvas = detect(gray, frame)\n",
    "    \n",
    "    # # display the outputs \n",
    "    cv2.imshow('Video', canvas)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "        break\n",
    "        \n",
    "video_capture.release() # We turn the webcam off.\n",
    "cv2.destroyAllWindows() # We destroy all the windows inside which the images were displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detecting emotions from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haarcascades from the opencv repo itself\n",
    "# https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_cascade = cv2.CascadeClassifier(\"./data/haarcascade_smile.xml\")\n",
    "\n",
    "def detect_smile(gray, frame):\n",
    "    faces = smile_cascade.detectMultiScale(image=gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img=frame, pt1=(x, y), pt2=(x+w, y+h), color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_frame = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(image=roi_gray, scaleFactor=1.1, minNeighbors=22)\n",
    "        \n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(img=roi_frame, pt1=(ex, ey), pt2=(ex+ew, ey+eh), color=(0, 255, 0), thickness=2)\n",
    "            \n",
    "        smiles = smile_cascade.detectMultiScale(image=roi_gray, scaleFactor=1.7, minNeighbors=22)\n",
    "        \n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(img=roi_frame, pt1=(sx, sy), pt2=(sx+sw, sy+sh), color=(0, 0, 255), thickness=2)      \n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # get the last frame from the webcam\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    # We do some colour transformation\n",
    "    # black and white version of the image \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # we get the output of our detect function\n",
    "    canvas = detect_smile(gray, frame)\n",
    "    \n",
    "    # # display the outputs \n",
    "    cv2.imshow('Video', canvas)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "        break\n",
    "        \n",
    "video_capture.release() # We turn the webcam off.\n",
    "cv2.destroyAllWindows() # We destroy all the windows inside which the images were displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Single Shot Multibox Detection (SSD) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How SSD is different ?\n",
    "# Image is just seen or basically input once. Patches are not searched iteratively in image with different sizes of patches.\n",
    "\n",
    "# SSD will break down imagein segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
